{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "332ea0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f220c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import causallearn\n",
    "\n",
    "from causallearn.utils.GraphUtils import GraphUtils\n",
    "import matplotlib.image as mpimg\n",
    "import io\n",
    "import os\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib.colors import Normalize\n",
    "import matplotlib as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88cdbd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DDQN network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=64):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.relu(self.fc1(state))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "402652be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prelare offline data, experiment: bridge, different pktsize-corresponding the different ddioway?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2ca1ef8",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '..\\\\bridge\\\\datasets\\\\combined_64.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m csv_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mbridge\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mcombined_64.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Read the CSV file\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '..\\\\bridge\\\\datasets\\\\combined_64.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to your CSV file\n",
    "csv_file = f'..\\\\bridge\\\\datasets\\\\combined_64.csv'\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(csv_file, header=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "220cdd2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'MultiIndex' has no attribute 'from_pruduce'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m states_set \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMultiIndex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pruduce\u001b[49m([df\u001b[38;5;241m.\u001b[39mcolumns,[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbridge-pcm_Socket 0-IPC\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbridge-pcm_Socket 0-L3MISS\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbridge-pcm_Socket 0-L3HIT\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      2\u001b[0m                                                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpcm-pcie_new_skt-0_PCIRdCur\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpcm-pcie_new_skt-0_ItoM\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m                                                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatency\u001b[39m\u001b[38;5;124m\"\u001b[39m,]], names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOriginal Column\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSubcolumn\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m new_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(states_set)\n\u001b[0;32m      5\u001b[0m new_df\u001b[38;5;241m.\u001b[39minfo()\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'MultiIndex' has no attribute 'from_pruduce'"
     ]
    }
   ],
   "source": [
    "states_set = pd.MultiIndex.from_pruduce([df.columns,[\"bridge-pcm_Socket 0-IPC\",\"bridge-pcm_Socket 0-L3MISS\",\"bridge-pcm_Socket 0-L3HIT\",\n",
    "                                                     \"pcm-pcie_new_skt-0_PCIRdCur\",\"pcm-pcie_new_skt-0_ItoM\",\n",
    "                                                     \"input_rate\",\"output_rate\",\"latency\",]], names=['Original Column', 'Subcolumn'])\n",
    "new_df = pd.DataFrame(states_set)\n",
    "new_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ff10f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience replay memory\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\"))(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e6af73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to select an action\n",
    "def select_action(state, online_net, epsilon):\n",
    "    if random.random() > epsilon:\n",
    "        with torch.no_grad():\n",
    "            return online_net(state).max(1)[1].data.numpy()\n",
    "        action = action[0] if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE) \n",
    "        exploit_count +=1\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(action_size)]], dtype=torch.long)\n",
    "        explore_count +=1\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c19d77a",
   "metadata": {},
   "source": [
    "# Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1718276a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ddqn(env,online_net,target_net, optimizer, memory, episodes, batch_size, target_update):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        epsilon = max(epsilon_end, epsilon_start * (epsilon_decay ** episode))\n",
    "        \n",
    "        # Update the target network\n",
    "        if episode % target_update == 0:\n",
    "            target_net.load_state_dict(online_net.state_dict())\n",
    "            \n",
    "        for t in count():\n",
    "            action = action = select_action(state, online_net, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.push(state,action,next_state,reward)\n",
    "            \n",
    "            if len(memory) > batch_size:\n",
    "                transitions = memory.sample(batch_size)\n",
    "                batch = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\"))(*zip(*transitions))\n",
    "                # Prepare the batch for training\n",
    "                non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), dtype=torch.bool)\n",
    "                non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "                state_batch = torch.cat(batch.state)\n",
    "                action_batch = torch.cat(batch.action)\n",
    "                reward_batch = torch.tensor(batch.reward)\n",
    "\n",
    "                # Compute Q values\n",
    "                state_action_values = online_net(state_batch).gather(1, action_batch)\n",
    "                next_state_values = torch.zeros(batch_size)\n",
    "                next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "                expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
    "\n",
    "                # Compute loss\n",
    "                loss = nn.functional.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "                \n",
    "                # Optimize the model\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            if done:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a555ddf9",
   "metadata": {},
   "source": [
    "# Running Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "246fac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ddqn(env, online_net):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        action = online_net(state).max(1)[1].view(1,1)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_reward +=reward\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6695fa3",
   "metadata": {},
   "source": [
    "# Environment Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20680604",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ServerEnvironment:\n",
    "    # para\n",
    "    MAX_STEPS = 1000\n",
    "    DESIRED_THROUGHPUT = 0.95\n",
    "    def __init__(self,num_nf_instance,num_cores,num_llc_ways):\n",
    "        self.num_nf_instances = num_nf_instances\n",
    "        self.num_cores = num_cores\n",
    "        self.num_llc_ways=num_llc_ways\n",
    "        self.state=self._initialize_state()\n",
    "        \n",
    "    def _initialize_state(self):\n",
    "        #llc\n",
    "        base_allocation = self.num_llc_ways // self.num_nf_instances\n",
    "        remainder = self.num_llc_ways % self.num_nf_instances\n",
    "        default_llc_partitions = [base_allocation] * self.num_nf_instances\n",
    "        for i in range(remainder):\n",
    "            default_llc_partitions[i] += 1\n",
    "            \n",
    "        #dma    \n",
    "        default_dma_buffer_size = 1024\n",
    "        \n",
    "        # traffic \n",
    "        \n",
    "        self.state = {\n",
    "            'llc_partitions':default_llc_partitions,\n",
    "            'dma_buffer_size':default_dma_buffer_size\n",
    "        }\n",
    "        return self.state\n",
    "        \n",
    "    def step(self,action):\n",
    "        new_state = self_apply_action(action)\n",
    "        reward = self_calculate_reward(new_state)\n",
    "        done = self._check_termination_condition(new_state)\n",
    "        return new_state, reward, done\n",
    "    \n",
    "    def _apply_action(self, action):\n",
    "        # action is a dictionary with keys corresponding to what needs to be changed\n",
    "        # action = {'llc_change': [0.1, -0.1, 0, 0],'dma_change': 256}\n",
    "        \n",
    "        #Adjust LLC partition\n",
    "        if 'llc_change' in action:\n",
    "            for i,change in enumerate(action['llc_change']):\n",
    "                self.state['llc_partitions'][i]+= change\n",
    "            self.state['llc_partitions'] = [max(0,alloc) for alloc in self.state['llc_partitions']]\n",
    "            total_alloc = sum(self.state['llc_partitions'])\n",
    "            if total_alloc > 0:\n",
    "                self.state['llc_partitions'] =[alloc / total_alloc for alloc in self.state['llc_partitions']]\n",
    "            else:\n",
    "                self.state['llc_paritions'] = [1./len(self.state['llc_partitions'])]*len(self.state['llc_partitions'])\n",
    "        \n",
    "        #Adjust DMA buffer size\n",
    "        if 'dma_change' in action:\n",
    "            new_buffer_size = self.state['dma_buffer_size'] + action['dma_change']\n",
    "            valid_buffer_sizes = [64, 256, 512, 1024, 2048]\n",
    "            self.state['dma_buffer_size'] = min(valid_buffer_sizes, key=lambda x: abs(x - new_buffer_size))\n",
    "   \n",
    "        return self.state\n",
    "\n",
    "    def _calculate_reward(self, state):\n",
    "        R = state['throughput']\n",
    "        T = state['input_rate']\n",
    "        R_i_list = state['nf_throughput'] # list of throughput for each NF\n",
    "        T_i_list = state['nf_inputrate'] # list of input rate for each NF\n",
    "        \n",
    "        r1 = R/T if T!=0 else 0\n",
    "        r2 = max((T_i-R_i)/T_i if T_i !=0 else 0 for R_i,T_i in zip(R_i_list, T_i_list))\n",
    "        eta1 = 1\n",
    "        eta2 = 0.1\n",
    "        r_a = eta1 *r1 - eta2 * r2\n",
    "        return r_a\n",
    "    def _check_ternimation_condition(self, state):\n",
    "        self.current_step +=1\n",
    "        if self.current_step >= self.MAX_STEP:\n",
    "            return True\n",
    "        elif state['throughout']>= DESIRED_THROUGHPUT:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    def reset(self):\n",
    "        # Reset the environment for a new episode\n",
    "        self.state = self._initialize_state()\n",
    "        return self.state     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9420efe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state: {'llc_partitions': [6, 5], 'dma_buffer_size': 1024}\n"
     ]
    }
   ],
   "source": [
    "# initialize environment\n",
    "num_nf_instances = 2\n",
    "num_llc_ways = 11\n",
    "num_cores = 20\n",
    "\n",
    "env= ServerEnvironment(num_nf_instances, num_cores, num_llc_ways)\n",
    "\n",
    "initial_state = env.reset()\n",
    "print(\"Initial state:\", initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1c914f",
   "metadata": {},
   "source": [
    "# Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50084460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "state_size = 13 # Number of state features\n",
    "action_size = 4 # Number of possible actions\n",
    "hidden_size = 64\n",
    "batch_size = 32\n",
    "gamma = 0.95  # Discount factor\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.01\n",
    "epsilon_decay = 0.995\n",
    "target_update = 20  # How frequently to update the target network\n",
    "memory_size = 10000\n",
    "learning_rate = 0.001\n",
    "capacity = 100\n",
    "episodes=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d05d21fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "DMA_buffer_space = [64,128,256,512,1024,2048]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33e9fa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DDQN networks and replay memory\n",
    "\n",
    "online_net = Net(state_size, action_size, hidden_size)\n",
    "target_net = Net(state_size, action_size, hidden_size)\n",
    "target_net.load_state_dict(online_net.state_dict())\n",
    "target_net.eval()  \n",
    "optimizer = optim.Adam(online_net.parameters(), lr=learning_rate)\n",
    "memory = ReplayMemory(memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b84dc53",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_ddqn() missing 1 required positional argument: 'target_update'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain_ddqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monline_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[0;32m      5\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(online_net\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolicy_net.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: train_ddqn() missing 1 required positional argument: 'target_update'"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_ddqn(env, online_net, target_net, optimizer, memory, episodes, batch_size)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(online_net.state_dict(), 'policy_net.pth')\n",
    "\n",
    "# Load the trained model for running\n",
    "online_net.load_state_dict(torch.load('policy_net.pth'))\n",
    "online_net.eval()  # Set to evaluation mode\n",
    "\n",
    "# Run the model\n",
    "total_reward = run_ddqn(env, online_net)\n",
    "print(f\"Total reward: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fcda45",
   "metadata": {},
   "outputs": [],
   "source": [
    "env= "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
