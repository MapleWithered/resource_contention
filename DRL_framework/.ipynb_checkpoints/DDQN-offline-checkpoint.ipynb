{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "332ea0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88cdbd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DDQN network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=64):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.relu(self.fc1(state))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ff10f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience replay memory\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\"))(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e6af73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to select an action\n",
    "def select_action(state, online_net, epsilon):\n",
    "    if random.random() > epsilon:\n",
    "        with torch.no_grad():\n",
    "            return online_net(state).max(1)[1].data.numpy()\n",
    "        action = action[0] if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE) \n",
    "        exploit_count +=1\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(action_size)]], dtype=torch.long)\n",
    "        explore_count +=1\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c19d77a",
   "metadata": {},
   "source": [
    "# Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1718276a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ddqn(env,online_net,target_net, optimizer, memory, episodes, batch_size, target_update):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        epsilon = max(epsilon_end, epsilon_start * (epsilon_decay ** episode))\n",
    "        \n",
    "        # Update the target network\n",
    "        if episode % target_update == 0:\n",
    "            target_net.load_state_dict(online_net.state_dict())\n",
    "            \n",
    "        for t in count():\n",
    "            action = action = select_action(state, online_net, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.push(state,action,next_state,reward)\n",
    "            \n",
    "            if len(memory) > batch_size:\n",
    "                transitions = memory.sample(batch_size)\n",
    "                batch = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\"))(*zip(*transitions))\n",
    "                # Prepare the batch for training\n",
    "                non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), dtype=torch.bool)\n",
    "                non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "                state_batch = torch.cat(batch.state)\n",
    "                action_batch = torch.cat(batch.action)\n",
    "                reward_batch = torch.tensor(batch.reward)\n",
    "\n",
    "                # Compute Q values\n",
    "                state_action_values = online_net(state_batch).gather(1, action_batch)\n",
    "                next_state_values = torch.zeros(batch_size)\n",
    "                next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "                expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
    "\n",
    "                # Compute loss\n",
    "                loss = nn.functional.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "                \n",
    "                # Optimize the model\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            if done:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a555ddf9",
   "metadata": {},
   "source": [
    "# Running Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "246fac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ddqn(env, online_net):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        action = online_net(state).max(1)[1].view(1,1)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_reward +=reward\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6695fa3",
   "metadata": {},
   "source": [
    "# Environment Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20680604",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ServerEnvironment:\n",
    "    # para\n",
    "    MAX_STEPS = 1000\n",
    "    DESIRED_THROUGHPUT = 0.95\n",
    "    def __init__(self,num_nf_instance,num_cores,num_llc_ways):\n",
    "        self.num_nf_instances = num_nf_instances\n",
    "        self.num_cores = num_cores\n",
    "        self.num_llc_ways=num_llc_ways\n",
    "        self.state=self._initialize_state()\n",
    "        \n",
    "    def _initialize_state(self):\n",
    "        #llc\n",
    "        base_allocation = self.num_llc_ways // self.num_nf_instances\n",
    "        remainder = self.num_llc_ways % self.num_nf_instances\n",
    "        default_llc_partitions = [base_allocation] * self.num_nf_instances\n",
    "        for i in range(remainder):\n",
    "            default_llc_partitions[i] += 1\n",
    "            \n",
    "        #dma    \n",
    "        default_dma_buffer_size = 1024\n",
    "        \n",
    "        # traffic \n",
    "        \n",
    "        self.state = {\n",
    "            'llc_partitions':default_llc_partitions,\n",
    "            'dma_buffer_size':default_dma_buffer_size\n",
    "        }\n",
    "        return self.state\n",
    "        \n",
    "    def step(self,action):\n",
    "        new_state = self_apply_action(action)\n",
    "        reward = self_calculate_reward(new_state)\n",
    "        done = self._check_termination_condition(new_state)\n",
    "        return new_state, reward, done\n",
    "    \n",
    "    def _apply_action(self, action):\n",
    "        # action is a dictionary with keys corresponding to what needs to be changed\n",
    "        # action = {'llc_change': [0.1, -0.1, 0, 0],'dma_change': 256}\n",
    "        \n",
    "        #Adjust LLC partition\n",
    "        if 'llc_change' in action:\n",
    "            for i,change in enumerate(action['llc_change']):\n",
    "                self.state['llc_partitions'][i]+= change\n",
    "            self.state['llc_partitions'] = [max(0,alloc) for alloc in self.state['llc_partitions']]\n",
    "            total_alloc = sum(self.state['llc_partitions'])\n",
    "            if total_alloc > 0:\n",
    "                self.state['llc_partitions'] =[alloc / total_alloc for alloc in self.state['llc_partitions']]\n",
    "            else:\n",
    "                self.state['llc_paritions'] = [1./len(self.state['llc_partitions'])]*len(self.state['llc_partitions'])\n",
    "        \n",
    "        #Adjust DMA buffer size\n",
    "        if 'dma_change' in action:\n",
    "            new_buffer_size = self.state['dma_buffer_size'] + action['dma_change']\n",
    "            valid_buffer_sizes = [64, 256, 512, 1024, 2048]\n",
    "            self.state['dma_buffer_size'] = min(valid_buffer_sizes, key=lambda x: abs(x - new_buffer_size))\n",
    "   \n",
    "        return self.state\n",
    "\n",
    "    def _calculate_reward(self, state):\n",
    "        R = state['throughput']\n",
    "        T = state['input_rate']\n",
    "        R_i_list = state['nf_throughput'] # list of throughput for each NF\n",
    "        T_i_list = state['nf_inputrate'] # list of input rate for each NF\n",
    "        \n",
    "        r1 = R/T if T!=0 else 0\n",
    "        r2 = max((T_i-R_i)/T_i if T_i !=0 else 0 for R_i,T_i in zip(R_i_list, T_i_list))\n",
    "        eta1 = 1\n",
    "        eta2 = 0.1\n",
    "        r_a = eta1 *r1 - eta2 * r2\n",
    "        return r_a\n",
    "    def _check_ternimation_condition(self, state):\n",
    "        self.current_step +=1\n",
    "        if self.current_step >= self.MAX_STEP:\n",
    "            return True\n",
    "        elif state['throughout']>= DESIRED_THROUGHPUT:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    def reset(self):\n",
    "        # Reset the environment for a new episode\n",
    "        self.state = self._initialize_state()\n",
    "        return self.state     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9420efe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state: {'llc_partitions': [6, 5], 'dma_buffer_size': 1024}\n"
     ]
    }
   ],
   "source": [
    "# initialize environment\n",
    "num_nf_instances = 2\n",
    "num_llc_ways = 11\n",
    "num_cores = 20\n",
    "\n",
    "env= ServerEnvironment(num_nf_instances, num_cores, num_llc_ways)\n",
    "\n",
    "initial_state = env.reset()\n",
    "print(\"Initial state:\", initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1c914f",
   "metadata": {},
   "source": [
    "# Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50084460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "state_size = 13 # Number of state features\n",
    "action_size = 4 # Number of possible actions\n",
    "hidden_size = 64\n",
    "batch_size = 32\n",
    "gamma = 0.95  # Discount factor\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.01\n",
    "epsilon_decay = 0.995\n",
    "target_update = 20  # How frequently to update the target network\n",
    "memory_size = 10000\n",
    "learning_rate = 0.001\n",
    "capacity = 100\n",
    "episodes=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d05d21fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "DMA_buffer_space = [64,128,256,512,1024,2048]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33e9fa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DDQN networks and replay memory\n",
    "\n",
    "online_net = Net(state_size, action_size, hidden_size)\n",
    "target_net = Net(state_size, action_size, hidden_size)\n",
    "target_net.load_state_dict(online_net.state_dict())\n",
    "target_net.eval()  \n",
    "optimizer = optim.Adam(online_net.parameters(), lr=learning_rate)\n",
    "memory = ReplayMemory(memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b84dc53",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_ddqn() missing 1 required positional argument: 'target_update'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain_ddqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monline_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[0;32m      5\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(online_net\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolicy_net.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: train_ddqn() missing 1 required positional argument: 'target_update'"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_ddqn(env, online_net, target_net, optimizer, memory, episodes, batch_size)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(online_net.state_dict(), 'policy_net.pth')\n",
    "\n",
    "# Load the trained model for running\n",
    "online_net.load_state_dict(torch.load('policy_net.pth'))\n",
    "online_net.eval()  # Set to evaluation mode\n",
    "\n",
    "# Run the model\n",
    "total_reward = run_ddqn(env, online_net)\n",
    "print(f\"Total reward: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fcda45",
   "metadata": {},
   "outputs": [],
   "source": [
    "env= "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
